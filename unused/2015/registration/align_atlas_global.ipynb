{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Gordon\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utilities2015 failed: Traceback (most recent call last):\n",
      "  File \"/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AttributeError: __abstractmethods__\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from registration_utilities import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 303.   200.5  265.5]\n"
     ]
    }
   ],
   "source": [
    "# Load atlas volume\n",
    "\n",
    "# atlas_volume = bp.unpack_ndarray_file(os.path.join(volume_dir, 'MD589/volume_MD589_annotation_withOuterContour.bp'))\n",
    "atlas_volume = bp.unpack_ndarray_file(volume_dir + '/atlasVolume_icp.bp')\n",
    "\n",
    "atlas_ydim, atlas_xdim, atlas_zdim = atlas_volume.shape\n",
    "atlas_centroid = np.array([.5*atlas_xdim, .5*atlas_ydim, .5*atlas_zdim])\n",
    "print atlas_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "available_labels_sided = [labels_sided[i-1] for i in np.unique(atlas_volume) if i > 0]\n",
    "available_labels_unsided = set([labelMap_sidedToUnsided[name] for name in available_labels_sided ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load atlas: 2.046726 seconds\n"
     ]
    }
   ],
   "source": [
    "def parallel_where(name, num_samples=None):\n",
    "    global atlas_volume, labels_sided_indices\n",
    "    \n",
    "    w = np.where(atlas_volume == labels_sided_indices[name])\n",
    "    \n",
    "    if num_samples is not None:\n",
    "        n = len(w[0])\n",
    "        sample_indices = np.random.choice(range(n), min(num_samples, n), replace=False)\n",
    "        return np.c_[w[1][sample_indices].astype(np.int16), \n",
    "                     w[0][sample_indices].astype(np.int16), \n",
    "                     w[2][sample_indices].astype(np.int16)]\n",
    "    else:\n",
    "        return np.c_[w[1].astype(np.int16), w[0].astype(np.int16), w[2].astype(np.int16)]\n",
    "    \n",
    "t = time.time()\n",
    "\n",
    "atlas_nzs = Parallel(n_jobs=16)(delayed(parallel_where)(name_s, num_samples=int(1e5)) for name_s in available_labels_sided)\n",
    "atlas_nzs = dict(zip(available_labels_sided, atlas_nzs))\n",
    "\n",
    "sys.stderr.write('load atlas: %f seconds\\n' % (time.time() - t)) #~ 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Center the nonzero voxels around atlas centroid\n",
    "pts_centered = {name: (np.concatenate([atlas_nzs[n] for n in labelMap_unsidedToSided[name]]) - atlas_centroid).astype(np.int16) \n",
    "                         for name in available_labels_unsided}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set weights of structures\n",
    "label_weights = {name: .1 if name == 'outerContour' else 1. for name in available_labels_unsided}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_score_and_gradient(T):\n",
    "    \"\"\"Compute score and gradient.\"\"\"\n",
    "    global pts_centered\n",
    "    \n",
    "    score = 0\n",
    "    dMdA = np.zeros((12,))\n",
    "    \n",
    "    for name in available_labels_unsided:\n",
    "#         t1 = time.time()\n",
    "    \n",
    "        pts_prime = transform_points(T, pts_centered=pts_centered[name], c_prime=test_centroid)\n",
    "        \n",
    "        xs_prime, ys_prime, zs_prime = pts_prime.T.astype(np.int16)\n",
    "\n",
    "        valid = (xs_prime >= 0) & (ys_prime >= 0) & (zs_prime >= 0) & \\\n",
    "                (xs_prime < test_xdim) & (ys_prime < test_ydim) & (zs_prime < test_zdim)\n",
    "                   \n",
    "        if np.count_nonzero(valid) > 0:\n",
    "\n",
    "            xs_prime_valid = xs_prime[valid]\n",
    "            ys_prime_valid = ys_prime[valid]\n",
    "            zs_prime_valid = zs_prime[valid]\n",
    "            \n",
    "            voxel_probs_valid = volume2_allLabels[name][ys_prime_valid, xs_prime_valid, zs_prime_valid] / 1e4\n",
    "\n",
    "            score += label_weights[name] * voxel_probs_valid.sum()\n",
    "            \n",
    "            Sx = dSdxyz[name][0, ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Sy = dSdxyz[name][1, ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Sz = dSdxyz[name][2, ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            \n",
    "            dxs, dys, dzs = pts_centered[name][valid].T\n",
    "\n",
    "            q = np.c_[Sx*dxs, Sx*dys, Sx*dzs, Sx, \n",
    "                          Sy*dxs, Sy*dys, Sy*dzs, Sy,\n",
    "                          Sz*dxs, Sz*dys, Sz*dzs, Sz]        \n",
    "            \n",
    "            dMdA += label_weights[name] * q.sum(axis=0)\n",
    "            \n",
    "            del voxel_probs_valid, q, Sx, Sy, Sz, dxs, dys, dzs, xs_prime_valid, ys_prime_valid, zs_prime_valid\n",
    "        \n",
    "#         sys.stderr.write('########### %s: %f seconds\\n' % (labels[l], time.time() - t1))\n",
    "        \n",
    "        del valid, xs_prime, ys_prime, zs_prime, pts_prime\n",
    "        \n",
    "    return score, dMdA\n",
    "\n",
    "def compute_score(T):\n",
    "    \"\"\"Compute score.\"\"\"    \n",
    "    score = 0\n",
    "    for name in available_labels_unsided:\n",
    "        \n",
    "        pts_prime = transform_points(T, pts_centered=pts_centered[name], c_prime=test_centroid)\n",
    "    \n",
    "        xs_prime, ys_prime, zs_prime = pts_prime.T.astype(np.int16)\n",
    "        \n",
    "        valid = (xs_prime >= 0) & (ys_prime >= 0) & (zs_prime >= 0) & \\\n",
    "            (xs_prime < test_xdim) & (ys_prime < test_ydim) & (zs_prime < test_zdim)\n",
    "            \n",
    "        voxel_probs_valid = volume2_allLabels[name][ys_prime[valid], xs_prime[valid], zs_prime[valid]] / 1e4\n",
    "\n",
    "        score += label_weights[name] * voxel_probs_valid.sum()\n",
    "                \n",
    "        del voxel_probs_valid, valid, xs_prime, ys_prime, zs_prime, pts_prime\n",
    "                \n",
    "    return score\n",
    "\n",
    "def compute_score_gradient(T):\n",
    "    \"\"\"Compute gradient of score.\"\"\"\n",
    "\n",
    "    dMdA = np.zeros((12,))\n",
    "\n",
    "    for name in available_labels_unsided:\n",
    "#       \n",
    "        pts_prime = transform_points(T, pts_centered=pts_centered[name], c_prime=test_centroid)\n",
    "\n",
    "        xs_prime, ys_prime, zs_prime = pts_prime.T.astype(np.int16)\n",
    "\n",
    "        valid = (xs_prime >= 0) & (ys_prime >= 0) & (zs_prime >= 0) & \\\n",
    "            (xs_prime < test_xdim) & (ys_prime < test_ydim) & (zs_prime < test_zdim)\n",
    "            \n",
    "        if np.count_nonzero(valid) > 0:\n",
    "            \n",
    "            xs_prime_valid = xs_prime[valid]\n",
    "            ys_prime_valid = ys_prime[valid]\n",
    "            zs_prime_valid = zs_prime[valid]\n",
    "            \n",
    "            Sx = dSdxyz[name][0, ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Sy = dSdxyz[name][1, ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Sz = dSdxyz[name][2, ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "               \n",
    "            dxs, dys, dzs = pts_centered[name][valid].T\n",
    "                        \n",
    "            dMdA += label_weights[name] * np.c_[Sx*dxs, Sx*dys, Sx*dzs, Sx, \n",
    "                          Sy*dxs, Sy*dys, Sy*dzs, Sy,\n",
    "                          Sz*dxs, Sz*dys, Sz*dzs, Sz].sum(axis=0)\n",
    "            \n",
    "    return dMdA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_score_hessian(T):\n",
    "    \n",
    "    Tm = np.reshape(T, (3,4))\n",
    "    tx, ty, tz = Tm[:, 3]\n",
    "    A = Tm[:, :3]\n",
    "\n",
    "    d2MdT2 = np.zeros((12, 12))\n",
    "    \n",
    "    for l in range(1, n_labels):\n",
    "\n",
    "        xs_prime, ys_prime, zs_prime = (np.dot(A, ds[l-1]) + \\\n",
    "                                        np.asarray([tx + test_cx, \n",
    "                                                    ty + test_cy, \n",
    "                                                    tz + test_cz])[:,np.newaxis]).astype(np.int)\n",
    "\n",
    "        valid = (xs_prime >= 0) & (ys_prime >= 0) & (zs_prime >= 0) & \\\n",
    "            (xs_prime < test_xdim) & (ys_prime < test_ydim) & (zs_prime < test_zdim)\n",
    "\n",
    "        if np.count_nonzero(valid) > 0:\n",
    "\n",
    "            xs_prime_valid = xs_prime[valid]\n",
    "            ys_prime_valid = ys_prime[valid]\n",
    "            zs_prime_valid = zs_prime[valid]\n",
    "            \n",
    "            dxs, dys, dzs = ds[l-1][:, valid]\n",
    "\n",
    "            Sxx_full, Sxy_full, Sxz_full, Syx_full, Syy_full, Syz_full, Szx_full, Szy_full, Szz_full = d2Sdxyz2[l-1]\n",
    "            Sxx = Sxx_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Sxy = Sxy_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Sxz = Sxz_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Syx = Syx_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Syy = Syy_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Syz = Syz_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Szx = Szx_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Szy = Szy_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "            Szz = Szz_full[ys_prime_valid, xs_prime_valid, zs_prime_valid]\n",
    "\n",
    "            rx = np.c_[Sxx*dxs, Sxx*dys, Sxx*dzs, Sxx, Sxy*dxs, Sxy*dys, Sxy*dzs, Sxy, Sxz*dxs, Sxz*dys, Sxz*dzs, Sxz]\n",
    "            ry = np.c_[Syx*dxs, Syx*dys, Syx*dzs, Syx, Syy*dxs, Syy*dys, Syy*dzs, Syy, Syz*dxs, Syz*dys, Syz*dzs, Syz]\n",
    "            rz = np.c_[Szx*dxs, Szx*dys, Szx*dzs, Szx, Szy*dxs, Szy*dys, Szy*dzs, Szy, Szz*dxs, Szz*dys, Szz*dzs, Szz]\n",
    "            r1 = (rx*dxs[:,None]).sum(axis=0)\n",
    "            r2 = (rx*dys[:,None]).sum(axis=0)\n",
    "            r3 = (rx*dzs[:,None]).sum(axis=0)\n",
    "            r4 = rx.sum(axis=0)\n",
    "            r5 = (ry*dxs[:,None]).sum(axis=0)\n",
    "            r6 = (ry*dys[:,None]).sum(axis=0)\n",
    "            r7 = (ry*dzs[:,None]).sum(axis=0)\n",
    "            r8 = ry.sum(axis=0)\n",
    "            r9 = (rz*dxs[:,None]).sum(axis=0)\n",
    "            r10 = (rz*dys[:,None]).sum(axis=0)\n",
    "            r11 = (rz*dzs[:,None]).sum(axis=0)\n",
    "            r12 = rz.sum(axis=0)\n",
    "            \n",
    "            d2MdT2_l = np.vstack([r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12])\n",
    "    \n",
    "        d2MdT2 += d2MdT2_l\n",
    "    \n",
    "    return d2MdT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BFGS\n",
    "\n",
    "from scipy.optimize import fmin_bfgs, fmin_ncg, fmin_l_bfgs_b, fmin_cg\n",
    "\n",
    "import collections, functools\n",
    "\n",
    "def func_wrapper(f, cache_size=10):\n",
    "    evals = {}\n",
    "    last_points = collections.deque()\n",
    "\n",
    "    def get(pt, which):\n",
    "        s = pt.tostring() # get binary string of numpy array, to make it hashable\n",
    "        if s not in evals:\n",
    "            evals[s] = f(pt)\n",
    "            last_points.append(s)\n",
    "            if len(last_points) >= cache_size:\n",
    "                del evals[last_points.popleft()]\n",
    "        return evals[s][which]\n",
    "\n",
    "    return functools.partial(get, which=0), functools.partial(get, which=1), functools.partial(get, which=2)\n",
    "\n",
    "tx_best, ty_best, tz_best = params_best_upToNow\n",
    "T_best = np.r_[1,0,0, tx_best, 0,1,0, ty_best, 0,0,1, tz_best]\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# f_, fprime = func_wrapper(compute_score_and_gradient_minus)\n",
    "\n",
    "f_, fprime, fhess = func_wrapper(compute_score_and_gradient_and_hessian_minus)\n",
    "\n",
    "# res = fmin_ncg(f=f_, x0=T_best, fprime=fprime, fhess=fhess, maxiter=100, epsilon=1e-3, full_output=True)\n",
    "# res = fmin_ncg(f=compute_score_minus, x0=T_best, fprime=compute_score_gradient_minus, maxiter=10, epsilon=1e-2)\n",
    "\n",
    "res = fmin_cg(f=f_, x0=T_best, fprime=fprime, maxiter=10)\n",
    "\n",
    "# res = fmin_bfgs(f=f_, x0=T_best, fprime=fprime, maxiter=10)\n",
    "\n",
    "# res = fmin_l_bfgs_b(func=compute_score_minus, x0=T_best, maxiter=10, approx_grad=True)\n",
    "\n",
    "sys.stderr.write('optimize: %f seconds\\n' % (time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "import numdifftools as nd\n",
    "g = nd.Gradient(compute_score, step=np.r_[1e-1, 1e-1, 1e-1, 5,\n",
    "                                          1e-1, 1e-1, 1e-1, 5,\n",
    "                                          1e-1, 1e-1, 1e-1, 5])\n",
    "g(T_final).reshape((3,4))\n",
    "\n",
    "h = nd.Hessdiag(compute_score, step=np.r_[1e-1, 1e-1, 1e-1, 5,\n",
    "                                          1e-1, 1e-1, 1e-1, 5,\n",
    "                                          1e-1, 1e-1, 1e-1, 5])\n",
    "h(T_final).reshape((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load score volumes: 27.534788 seconds\n",
      "load gradient RtTg: 9.887150 seconds\n",
      "load gradient VLL: 9.731063 seconds\n",
      "load gradient Tz: 6.745456 seconds\n",
      "load gradient LC: 9.954948 seconds\n",
      "load gradient 7N: 11.525541 seconds\n",
      "load gradient Amb: 9.783488 seconds\n",
      "load gradient 6N: 21.602796 seconds\n",
      "load gradient AP: 21.822907 seconds\n",
      "load gradient 5N: 25.945059 seconds\n",
      "load gradient 12N: 23.566005 seconds\n",
      "load gradient 7n: 19.242286 seconds\n",
      "load gradient R: 17.520688 seconds\n",
      "load gradient Pn: 23.521699 seconds\n",
      "load gradient LRt: 24.501832 seconds\n",
      "overall: 235.356896 seconds\n",
      "INFO:__main__:grid search iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_xdim, test_ydim, test_zdim: 838 460 454\n",
      "test_centroid: [ 419.  230.  227.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grid search: 27.620252 seconds\n",
      "INFO:__main__:0.000000 46.801414\n",
      "INFO:__main__:-111.894222 37.912516 -15.082959\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:grid search iteration 1\n",
      "grid search: 21.448890 seconds\n",
      "INFO:__main__:46.801414 48.977577\n",
      "INFO:__main__:-94.038851 43.683430 15.596275\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:grid search iteration 2\n",
      "grid search: 16.866659 seconds\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:grid search iteration 3\n",
      "grid search: 13.159560 seconds\n",
      "INFO:__main__:48.977577 50.862104\n",
      "INFO:__main__:-95.671343 28.265419 -11.426957\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:grid search iteration 4\n",
      "grid search: 10.166200 seconds\n",
      "INFO:__main__:50.862104 54.893005\n",
      "INFO:__main__:-91.616441 36.602228 -1.911909\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 0\n",
      "INFO:__main__:score: 54.893005\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 1\n",
      "INFO:__main__:score: 37.657516\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 2\n",
      "INFO:__main__:score: 64.124519\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 3\n",
      "INFO:__main__:score: 67.099365\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 4\n",
      "INFO:__main__:score: 74.031803\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 5\n",
      "INFO:__main__:score: 79.685898\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 6\n",
      "INFO:__main__:score: 80.387840\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 7\n",
      "INFO:__main__:score: 80.374737\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 8\n",
      "INFO:__main__:score: 82.480625\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 9\n",
      "INFO:__main__:score: 82.758865\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 10\n",
      "INFO:__main__:score: 83.651234\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 11\n",
      "INFO:__main__:score: 83.959286\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 12\n",
      "INFO:__main__:score: 84.642204\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 13\n",
      "INFO:__main__:score: 84.513626\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 14\n",
      "INFO:__main__:score: 84.934338\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 15\n",
      "INFO:__main__:score: 84.556435\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 16\n",
      "INFO:__main__:score: 84.812309\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 17\n",
      "INFO:__main__:score: 83.846382\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 18\n",
      "INFO:__main__:score: 84.125366\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 19\n",
      "INFO:__main__:score: 82.907890\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 20\n",
      "INFO:__main__:score: 84.373547\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 21\n",
      "INFO:__main__:score: 83.795685\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 22\n",
      "INFO:__main__:score: 84.610077\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 23\n",
      "INFO:__main__:score: 84.386726\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 24\n",
      "INFO:__main__:score: 85.037327\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 25\n",
      "INFO:__main__:score: 84.744637\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 26\n",
      "INFO:__main__:score: 85.185062\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 27\n",
      "INFO:__main__:score: 84.977879\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 28\n",
      "INFO:__main__:score: 85.308342\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 29\n",
      "INFO:__main__:score: 85.054081\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 30\n",
      "INFO:__main__:score: 85.390350\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 31\n",
      "INFO:__main__:score: 85.194592\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 32\n",
      "INFO:__main__:score: 85.492188\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 33\n",
      "INFO:__main__:score: 85.296944\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 34\n",
      "INFO:__main__:score: 85.537224\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 35\n",
      "INFO:__main__:score: 85.355400\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 36\n",
      "INFO:__main__:score: 85.579124\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 37\n",
      "INFO:__main__:score: 85.423443\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 38\n",
      "INFO:__main__:score: 85.616425\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 39\n",
      "INFO:__main__:score: 85.520290\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 40\n",
      "INFO:__main__:score: 85.656441\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 41\n",
      "INFO:__main__:score: 85.575924\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 42\n",
      "INFO:__main__:score: 85.700508\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 43\n",
      "INFO:__main__:score: 85.615749\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 44\n",
      "INFO:__main__:score: 85.707474\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 45\n",
      "INFO:__main__:score: 85.688000\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 46\n",
      "INFO:__main__:score: 85.744595\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 47\n",
      "INFO:__main__:score: 85.727875\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 48\n",
      "INFO:__main__:score: 85.764202\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 49\n",
      "INFO:__main__:score: 85.716248\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 50\n",
      "INFO:__main__:score: 85.773933\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 51\n",
      "INFO:__main__:score: 85.749401\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 52\n",
      "INFO:__main__:score: 85.798489\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 53\n",
      "INFO:__main__:score: 85.745422\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 54\n",
      "INFO:__main__:score: 85.828766\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 55\n",
      "INFO:__main__:score: 85.765064\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 56\n",
      "INFO:__main__:score: 85.847343\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 57\n",
      "INFO:__main__:score: 85.736813\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 58\n",
      "INFO:__main__:score: 85.851318\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 59\n",
      "INFO:__main__:score: 85.746525\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 60\n",
      "INFO:__main__:score: 85.840572\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 61\n",
      "INFO:__main__:score: 85.680908\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 62\n",
      "INFO:__main__:score: 85.772369\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 63\n",
      "INFO:__main__:score: 85.533173\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 64\n",
      "INFO:__main__:score: 85.691406\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 65\n",
      "INFO:__main__:score: 85.382465\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 66\n",
      "INFO:__main__:score: 85.555550\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 67\n",
      "INFO:__main__:score: 85.269020\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 68\n",
      "INFO:__main__:score: 85.552666\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 69\n",
      "INFO:__main__:score: 85.374245\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 70\n",
      "INFO:__main__:score: 85.571449\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 71\n",
      "INFO:__main__:score: 85.410385\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 72\n",
      "INFO:__main__:score: 85.641911\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 73\n",
      "INFO:__main__:score: 85.552734\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 74\n",
      "INFO:__main__:score: 85.713245\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 75\n",
      "INFO:__main__:score: 85.590199\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 76\n",
      "INFO:__main__:score: 85.765892\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 77\n",
      "INFO:__main__:score: 85.691936\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 78\n",
      "INFO:__main__:score: 85.746372\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 79\n",
      "INFO:__main__:score: 85.720242\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 80\n",
      "INFO:__main__:score: 85.754154\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 81\n",
      "INFO:__main__:score: 85.716351\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 82\n",
      "INFO:__main__:score: 85.747383\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 83\n",
      "INFO:__main__:score: 85.729996\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 84\n",
      "INFO:__main__:score: 85.775639\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 85\n",
      "INFO:__main__:score: 85.746803\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 86\n",
      "INFO:__main__:score: 85.793179\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 87\n",
      "INFO:__main__:score: 85.752678\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 88\n",
      "INFO:__main__:score: 85.818550\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 89\n",
      "INFO:__main__:score: 85.808491\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 90\n",
      "INFO:__main__:score: 85.807827\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 91\n",
      "INFO:__main__:score: 85.819248\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 92\n",
      "INFO:__main__:score: 85.815701\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 93\n",
      "INFO:__main__:score: 85.828045\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 94\n",
      "INFO:__main__:score: 85.827362\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 95\n",
      "INFO:__main__:score: 85.845642\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 96\n",
      "INFO:__main__:score: 85.867451\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 97\n",
      "INFO:__main__:score: 85.851486\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 98\n",
      "INFO:__main__:score: 85.870377\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 99\n",
      "INFO:__main__:score: 85.831982\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 100\n",
      "INFO:__main__:score: 85.862476\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 101\n",
      "INFO:__main__:score: 85.838791\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 102\n",
      "INFO:__main__:score: 85.886925\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 103\n",
      "INFO:__main__:score: 85.825172\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 104\n",
      "INFO:__main__:score: 85.898529\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 105\n",
      "INFO:__main__:score: 85.830997\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 106\n",
      "INFO:__main__:score: 85.898525\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 107\n",
      "INFO:__main__:score: 85.842705\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 108\n",
      "INFO:__main__:score: 85.891613\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 109\n",
      "INFO:__main__:score: 85.823288\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 110\n",
      "INFO:__main__:score: 85.899479\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 111\n",
      "INFO:__main__:score: 85.826141\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 112\n",
      "INFO:__main__:score: 85.878887\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 113\n",
      "INFO:__main__:score: 85.848618\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 114\n",
      "INFO:__main__:score: 85.876987\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 115\n",
      "INFO:__main__:score: 85.832935\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 116\n",
      "INFO:__main__:score: 85.887737\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 117\n",
      "INFO:__main__:score: 85.832027\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 118\n",
      "INFO:__main__:score: 85.894585\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 119\n",
      "INFO:__main__:score: 85.838898\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 120\n",
      "INFO:__main__:score: 85.879936\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 121\n",
      "INFO:__main__:score: 85.832973\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 122\n",
      "INFO:__main__:score: 85.903389\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 123\n",
      "INFO:__main__:score: 85.831974\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 124\n",
      "INFO:__main__:score: 85.879936\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 125\n",
      "INFO:__main__:score: 85.842747\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 126\n",
      "INFO:__main__:score: 85.879879\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 127\n",
      "INFO:__main__:score: 85.837887\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 128\n",
      "INFO:__main__:score: 85.895546\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 129\n",
      "INFO:__main__:score: 85.843693\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 130\n",
      "INFO:__main__:score: 85.877914\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 131\n",
      "INFO:__main__:score: 85.856400\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 132\n",
      "INFO:__main__:score: 85.878960\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 133\n",
      "INFO:__main__:score: 85.865189\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 134\n",
      "INFO:__main__:score: 85.873051\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 135\n",
      "INFO:__main__:score: 85.864212\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 136\n",
      "INFO:__main__:score: 85.877064\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 137\n",
      "INFO:__main__:score: 85.834919\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 138\n",
      "INFO:__main__:score: 85.882851\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 139\n",
      "INFO:__main__:score: 85.851524\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 140\n",
      "INFO:__main__:score: 85.885784\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 141\n",
      "INFO:__main__:score: 85.828060\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 142\n",
      "INFO:__main__:score: 85.887726\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 143\n",
      "INFO:__main__:score: 85.823231\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 144\n",
      "INFO:__main__:score: 85.901421\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 145\n",
      "INFO:__main__:score: 85.830063\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 146\n",
      "INFO:__main__:score: 85.890701\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 147\n",
      "INFO:__main__:score: 85.822243\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 148\n",
      "INFO:__main__:score: 85.902401\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 149\n",
      "INFO:__main__:score: 85.837833\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 150\n",
      "INFO:__main__:score: 85.910221\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 151\n",
      "INFO:__main__:score: 85.845665\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 152\n",
      "INFO:__main__:score: 85.899460\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 153\n",
      "INFO:__main__:score: 85.859341\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 154\n",
      "INFO:__main__:score: 85.904320\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 155\n",
      "INFO:__main__:score: 85.835903\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 156\n",
      "INFO:__main__:score: 85.910198\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 157\n",
      "INFO:__main__:score: 85.858345\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 158\n",
      "INFO:__main__:score: 85.897511\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 159\n",
      "INFO:__main__:score: 85.852486\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 160\n",
      "INFO:__main__:score: 85.889671\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 161\n",
      "INFO:__main__:score: 85.844681\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 162\n",
      "INFO:__main__:score: 85.912090\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 163\n",
      "INFO:__main__:score: 85.853481\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 164\n",
      "INFO:__main__:score: 85.918941\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 165\n",
      "INFO:__main__:score: 85.864223\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 166\n",
      "INFO:__main__:score: 85.912083\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 167\n",
      "INFO:__main__:score: 85.854458\n",
      "INFO:__main__:\n",
      "\n",
      "INFO:__main__:iteration 168\n",
      "INFO:__main__:score: 85.916973\n",
      "INFO:__main__:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x577cb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for stack in ['MD594', 'MD593', 'MD585', 'MD592', 'MD590', 'MD591', 'MD595', 'MD598', 'MD602']:\n",
    "for stack in ['MD589']:\n",
    "    \n",
    "    ################# Load Test Volume ######################\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    volume2_allLabels = {}\n",
    "\n",
    "    for name in available_labels_unsided:\n",
    "\n",
    "        if name == 'BackG':\n",
    "            continue\n",
    "\n",
    "        volume2_roi = bp.unpack_ndarray_file(os.path.join(volume_dir, '%(stack)s/%(stack)s_scoreVolume_%(label)s.bp' % \\\n",
    "                                                          {'stack': stack, 'label': name})).astype(np.float16)\n",
    "        volume2_allLabels[name] = volume2_roi\n",
    "        del volume2_roi\n",
    "\n",
    "    test_ydim, test_xdim, test_zdim = volume2_allLabels.values()[0].shape\n",
    "    test_centroid = np.r_[.5*test_xdim, .5*test_ydim, .5*test_zdim]\n",
    "\n",
    "    print 'test_xdim, test_ydim, test_zdim:', test_xdim, test_ydim, test_zdim\n",
    "    print 'test_centroid:', test_centroid\n",
    "\n",
    "    sys.stderr.write('load score volumes: %f seconds\\n' % (time.time() - t))\n",
    "\n",
    "    ###################### Load Gradient #####################\n",
    "\n",
    "    dSdxyz = {name: np.empty((3, test_ydim, test_xdim, test_zdim), dtype=np.float16) for name in available_labels_unsided}\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    for name in available_labels_unsided:\n",
    "\n",
    "        if name == 'BackG':\n",
    "            continue\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        dSdxyz[name][0] = bp.unpack_ndarray_file(volume_dir + '/%(stack)s/%(stack)s_scoreVolume_%(label)s_gx.bp' % {'stack':stack, 'label':name})\n",
    "        dSdxyz[name][1] = bp.unpack_ndarray_file(volume_dir + '/%(stack)s/%(stack)s_scoreVolume_%(label)s_gy.bp' % {'stack':stack, 'label':name})\n",
    "        dSdxyz[name][2] = bp.unpack_ndarray_file(volume_dir + '/%(stack)s/%(stack)s_scoreVolume_%(label)s_gz.bp' % {'stack':stack, 'label':name})\n",
    "\n",
    "        sys.stderr.write('load gradient %s: %f seconds\\n' % (name, time.time() - t)) # ~6s\n",
    "\n",
    "    sys.stderr.write('overall: %f seconds\\n' % (time.time() - t1)) # ~100s\n",
    "    \n",
    "    handler = logging.FileHandler(atlasAlignOptLogs_dir + '/%(stack)s_atlasAlignOpt.log' % {'stack': stack})\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    ################# Random Grid Search ######################\n",
    "\n",
    "    grid_search_iteration_number = 5\n",
    "    # grid_search_iteration_number = 1\n",
    "\n",
    "    params_best_upToNow = (0, 0, 0)\n",
    "    score_best_upToNow = 0\n",
    "\n",
    "    init_n = 1000\n",
    "\n",
    "    for iteration in range(grid_search_iteration_number):\n",
    "\n",
    "        logger.info('grid search iteration %d', iteration)\n",
    "\n",
    "        init_tx, init_ty, init_tz  = params_best_upToNow\n",
    "\n",
    "        n = int(init_n*np.exp(-iteration/3.))\n",
    "\n",
    "        sigma_tx = 300*np.exp(-iteration/3.)\n",
    "        sigma_ty = 300*np.exp(-iteration/3.)\n",
    "        sigma_tz = 100*np.exp(-iteration/3.)\n",
    "\n",
    "        tx_grid = init_tx + sigma_tx * (2 * np.random.random(n) - 1)\n",
    "        ty_grid = init_ty + sigma_ty * (2 * np.random.random(n) - 1)\n",
    "        tz_grid = init_tz + sigma_tz * (2 * np.random.random(n) - 1)\n",
    "\n",
    "        samples = np.c_[tx_grid, ty_grid, tz_grid]\n",
    "\n",
    "        import time\n",
    "\n",
    "        t = time.time()\n",
    "        scores = Parallel(n_jobs=8)(delayed(compute_score)(np.r_[1, 0, 0, tx, 0, 1, 0, ty, 0, 0, 1, tz])\n",
    "                                    for tx, ty, tz in samples)\n",
    "\n",
    "    #     scores = []\n",
    "    #     for tx, ty, tz in samples:\n",
    "    #         scores.append(compute_score([1, 0, 0, tx, 0, 1, 0, ty, 0, 0, 1, tz]))\n",
    "\n",
    "        sys.stderr.write('grid search: %f seconds\\n' % (time.time() - t)) # ~23s\n",
    "\n",
    "        score_best = np.max(scores)\n",
    "\n",
    "        tx_best, ty_best, tz_best = samples[np.argmax(scores)]\n",
    "\n",
    "        if score_best > score_best_upToNow:\n",
    "            logger.info('%f %f', score_best_upToNow, score_best)\n",
    "\n",
    "            score_best_upToNow = score_best\n",
    "            params_best_upToNow = tx_best, ty_best, tz_best\n",
    "\n",
    "            logger.info('%f %f %f', tx_best, ty_best, tz_best)\n",
    "\n",
    "        logger.info('\\n')\n",
    "        \n",
    "    ################# Gradient Descent ######################\n",
    "\n",
    "    lr1, lr2 = (10., 1e-1)\n",
    "    # lr1, lr2 = (1., 1e-3)\n",
    "\n",
    "    # auto_corr = .95\n",
    "\n",
    "    max_iter_num = 1000\n",
    "    fudge_factor = 1e-6 #for numerical stability\n",
    "    dMdA_historical = np.zeros((12,))\n",
    "\n",
    "    tx_best, ty_best, tz_best = params_best_upToNow\n",
    "    T_best = np.r_[1,0,0, tx_best, 0,1,0, ty_best, 0,0,1, tz_best]\n",
    "\n",
    "    lr = np.r_[lr2, lr2, lr2, lr1, lr2, lr2, lr2, lr1, lr2, lr2, lr2, lr1]\n",
    "\n",
    "    score_best = 0\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for iteration in range(max_iter_num):\n",
    "\n",
    "        logger.info('iteration %d', iteration)\n",
    "\n",
    "    #     t = time.time()\n",
    "        s, dMdA = compute_score_and_gradient(T_best)\n",
    "    #     sys.stderr.write('compute_score_and_gradient: %f seconds\\n' % (time.time() - t)) #~ 2s/iteration or ~.5s: 1e5 samples per landmark\n",
    "\n",
    "        dMdA_historical += dMdA**2\n",
    "    #     dMdA_historical = auto_corr * dMdA_historical + (1-auto_corr) * dMdA**2\n",
    "\n",
    "        dMdA_adjusted = dMdA / (fudge_factor + np.sqrt(dMdA_historical))\n",
    "\n",
    "        T_best += lr*dMdA_adjusted\n",
    "\n",
    "    #         logger.info('A: ' + ' '.join(['%f']*12) % tuple(A_best))\n",
    "    #         logger.info('dMdA adjusted: ' + ' '.join(['%f']*12) % tuple(dMdA_adjusted))\n",
    "\n",
    "        logger.info('score: %f', s)\n",
    "        scores.append(s)\n",
    "\n",
    "        logger.info('\\n')\n",
    "\n",
    "        history_len = 50\n",
    "        if iteration > 100:\n",
    "            if np.abs(np.mean(scores[iteration-history_len:iteration]) - \\\n",
    "                      np.mean(scores[iteration-2*history_len:iteration-history_len])) < 1e-1:\n",
    "                break\n",
    "\n",
    "        if s > score_best:\n",
    "    #             logger.info('Current best')\n",
    "            best_gradient_descent_params = T_best\n",
    "            score_best = s\n",
    "\n",
    "    plt.title('%s' % stack);\n",
    "    plt.plot(scores);\n",
    "    plt.show();\n",
    "    \n",
    "    del volume2_allLabels, dSdxyz\n",
    "    \n",
    "    ################# Save results ###############\n",
    "    \n",
    "#     np.save(atlasAlignOptLogs_dir + '/%(stack)s_scoreEvolutions.npy' % {'stack':stack}, scores)\n",
    "    \n",
    "#     create_if_not_exists(os.path.join(atlasAlignParams_dir + '/' + stack))\n",
    "#     with open(os.path.join(atlasAlignParams_dir, '%(stack)s/%(stack)s_3dAlignParams.txt' % {'stack':stack}), 'w') as f:\n",
    "\n",
    "#         f.writelines(' '.join(['%f']*len(best_gradient_descent_params)) % tuple(best_gradient_descent_params) + '\\n')\n",
    "#         f.write((' '.join(['%d']*3)+'\\n') % tuple([atlas_xdim, atlas_ydim, atlas_zdim]))\n",
    "#         f.write((' '.join(['%.1f']*3)+'\\n') % tuple(atlas_centroid))    \n",
    "#         f.write((' '.join(['%d']*3)+'\\n') % tuple([test_xdim, test_ydim, test_zdim]))\n",
    "#         f.write((' '.join(['%.1f']*3)+'\\n') % tuple(test_centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
